{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compiling\n",
    "\n",
    "This notebook creates pickled NLP file added to the directory. \n",
    "\n",
    "**Note**\n",
    "A prerun version of the file created in this notebook is available from AWS S3 server, the instructions of which are found in `Preprocessing.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting benepar\n",
      "  Using cached benepar-0.2.0.tar.gz (33 kB)\n",
      "Collecting nltk>=3.2\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: spacy>=2.0.9 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from benepar) (2.3.5)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.9.0-cp39-none-macosx_10_9_x86_64.whl (127.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 127.9 MB 182 kB/s  eta 0:00:01    |████████████▉                   | 51.2 MB 40.1 MB/s eta 0:00:02     |██████████████████████▎         | 88.9 MB 45.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch-struct>=0.5\n",
      "  Using cached torch_struct-0.5-py3-none-any.whl (34 kB)\n",
      "Collecting tokenizers>=0.9.4\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 34.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 41.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-3.17.3-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece>=0.1.91\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2021.4.4-cp39-cp39-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from nltk>=3.2->benepar) (4.61.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (1.20.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (2.25.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (7.4.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from spacy>=2.0.9->benepar) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.26.4)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 38.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (20.9)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from packaging->transformers[tokenizers,torch]>=4.2.2->benepar) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /opt/anaconda3/envs/CapstoneEnv/lib/python3.9/site-packages (from protobuf->benepar) (1.16.0)\n",
      "Building wheels for collected packages: benepar\n",
      "  Building wheel for benepar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37648 sha256=49c1c59bd264e9714ab733733b0ec4889f7cd2e880a17dca11e0c47179173b08\n",
      "  Stored in directory: /Users/megansorel/Library/Caches/pip/wheels/dc/9a/8b/5d4c83fde00b8d068594afa8dd3907809f592e019423df533f\n",
      "Successfully built benepar\n",
      "Installing collected packages: typing-extensions, regex, joblib, filelock, click, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, torch, torch-struct, sentencepiece, protobuf, nltk, benepar\n",
      "Successfully installed benepar-0.2.0 click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.12 joblib-1.0.1 nltk-3.6.2 protobuf-3.17.3 pyyaml-5.4.1 regex-2021.4.4 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.9.0 torch-struct-0.5 transformers-4.8.1 typing-extensions-3.10.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import benepar\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner'])\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure nlp for pickling\n",
    "\n",
    "config = nlp.config\n",
    "bytes_data = nlp.to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fb52f076310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking serialization of nlp pipeline \n",
    "\n",
    "lang_cls = spacy.util.get_lang_class(config[\"nlp\"][\"lang\"])\n",
    "nlp = lang_cls.from_config(config)\n",
    "nlp.from_bytes(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "# running the data through NLP pipeline\n",
    "text = [nlp(exc) for exc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the the main directory in the notebook \n",
    "\n",
    "file_to_store = open(\"train.pkl\", 'wb')\n",
    "\n",
    "pickle.dump(text, file_to_store)\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the work\n",
    "\n",
    "# file_to_read = open(\"train.pkl\", \"rb\")\n",
    "\n",
    "# loaded_object = pickle.load(file_to_read)\n",
    "\n",
    "# file_to_read.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
